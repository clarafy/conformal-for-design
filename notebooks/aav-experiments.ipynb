{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook reproduces the AAV design experiments whose results are shown in Fig 5.\n",
    "\n",
    "Variable name suffixes in the following cells denote array dimensions, where\n",
    "\n",
    "n: number of calibration and test data points  \n",
    "l: number of values of the inverse temperature, lambda  \n",
    "L: length of sequence  \n",
    "t: number of trials of sampling from test distribution, per lambda  \n",
    "s: number of samples from test distribution  \n",
    "m: number of calibration data points  \n",
    "m1: m + 1  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import time\n",
    "from importlib import reload\n",
    "module_path = os.path.abspath(os.path.join('..'))\n",
    "if module_path not in sys.path:\n",
    "    sys.path.append(module_path)\n",
    "    \n",
    "import numpy as np\n",
    "import scipy as sc\n",
    "from tensorflow import keras\n",
    "\n",
    "import assay\n",
    "import calibrate as cal"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load held-out data and parameters of test sequence distributions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 1000000 held-out test and calibration data points.\n"
     ]
    }
   ],
   "source": [
    "# load held-out data (calibration and test data)\n",
    "scale = 0.1\n",
    "d = np.load('../aav/test_and_calibration_data_scale{}.npz'.format(scale))\n",
    "seq_n = d['seq_n']  # list of strings\n",
    "y_n = d['y_n']      # true fitnesses\n",
    "n = y_n.size\n",
    "print('Loaded {} held-out test and calibration data points.'.format(n))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load parameters of test sequence distributions\n",
    "d = np.load('../aav/models/constrained_maxent_scale{}_052722.npz'.format(scale))\n",
    "\n",
    "# phitestnuc_lxLxk[i] is an (L, k) numpy array of unnormalized probabilities of a categorical distribution\n",
    "# over k = 4 nucleotides at each of L sequence positions,\n",
    "# corresponding to phi in Eq. 5 of Supp. Materials and Methods here:\n",
    "# https://www.biorxiv.org/content/10.1101/2021.11.02.467003v2.full\n",
    "phitestnuc_lxLxk = d['phitestnuc_lxLxk']\n",
    "\n",
    "# note that lambda in bioRxiv above corresponds to 1 / lambda for us\n",
    "lambda_l = (1 / d['temperature_l']).astype(int)\n",
    "meanpredfit_l = d['meanpredfit_l']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Construct confidence sets for designed sequences"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Compute predictions and scores for all held-out data (calibration and test data)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /home/clarafy/anaconda3/envs/tf-gpu/lib/python3.7/site-packages/tensorflow_core/python/ops/init_ops.py:97: calling GlorotUniform.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Call initializer instance with the dtype argument instead of passing it to the constructor\n",
      "WARNING:tensorflow:From /home/clarafy/anaconda3/envs/tf-gpu/lib/python3.7/site-packages/tensorflow_core/python/ops/init_ops.py:97: calling Zeros.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Call initializer instance with the dtype argument instead of passing it to the constructor\n",
      "WARNING:tensorflow:From /home/clarafy/anaconda3/envs/tf-gpu/lib/python3.7/site-packages/tensorflow_core/python/ops/resource_variable_ops.py:1630: calling BaseResourceVariable.__init__ (from tensorflow.python.ops.resource_variable_ops) with constraint is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "If using Keras pass *_constraint arguments to layers.\n",
      "WARNING:tensorflow:Output lambda_1 missing from loss dictionary. We assume this was done on purpose. The fit and evaluate APIs will not be expecting any data to be passed to lambda_1.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/clarafy/anaconda3/envs/tf-gpu/lib/python3.7/site-packages/tensorflow_core/python/keras/layers/core.py:895: UserWarning: aav.modeling is not loaded, but a Lambda layer uses it. It may cause errors.\n",
      "  , UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /home/clarafy/anaconda3/envs/tf-gpu/lib/python3.7/site-packages/tensorflow_core/python/ops/math_grad.py:1375: where (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.where in 2.0, which has the same broadcast rule as np.where\n"
     ]
    }
   ],
   "source": [
    "# load trained NN and predict for all held-out sequences\n",
    "datagen = assay.DataGenerator(seq_n)\n",
    "model = keras.models.load_model('../aav/models/h100_scale{}_0_052722.npy'.format(scale))\n",
    "pred_n = model.predict_generator(datagen).reshape(n)\n",
    "score_n = np.abs(pred_n - y_n)  # score with residual"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use rejection sampling to sample from test distribution, and construct split conformal confidence intervals for resulting designed sequences."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_trial = 500\n",
    "alpha = 0.1\n",
    "n_cal = 10000\n",
    "save_results = True\n",
    "savefile = '../aav/split-results.npz'\n",
    "\n",
    "# compute training likelihoods of all sequences\n",
    "logptrain_n = assay.get_loglikelihood(seq_n, assay.PNNKAA_LXK)\n",
    "\n",
    "n_lambda = phitestnuc_lxLxk.shape[0]\n",
    "cov_lxt = np.zeros([n_lambda, n_trial])\n",
    "avglen_lxt = np.zeros([n_lambda, n_trial])\n",
    "fracinf_lxt = np.zeros([n_lambda, n_trial])\n",
    "len_lxt = {(l, t): None for l, t in zip(range(n_lambda), range(n_trial))}\n",
    "fit_lxt = {(l, t): None for l, t in zip(range(n_lambda), range(n_trial))}\n",
    "\n",
    "for l in range(n_lambda - 1, -1, -1):\n",
    "    t0 = time.time()\n",
    "    print(\"Test distribution with lambda = {:.1f}\".format(lambda_l[l]))\n",
    "    \n",
    "    # compute acceptance probabilities for all sequences (for rejection sampling from test distribution)\n",
    "    paccept_n, logptest_n = assay.get_rejection_sampling_acceptance_probabilities(\n",
    "        seq_n, phitestnuc_lxLxk[l], logptrain_n)\n",
    "\n",
    "    # compute (unnormalized) weights for all data\n",
    "    w_n = np.exp(logptest_n - logptrain_n)\n",
    "\n",
    "    for t in range(n_trial):\n",
    "        \n",
    "        # partition held-out data into calibration data and test data\n",
    "        # (i.e., samples from proposal distribution for rejection sampling from test distribution)\n",
    "        shuffle_idx = np.random.permutation(n)\n",
    "        cal_idx, test_idx = shuffle_idx[: n_cal], shuffle_idx[n_cal :]\n",
    "        \n",
    "        # sample from test distribution using rejection sampling\n",
    "        testsamp_idx = assay.rejection_sample_from_test_distribution(paccept_n[test_idx])\n",
    "        n_test = testsamp_idx.size\n",
    "        if t == 0:\n",
    "            print(\"  On trial 0, sampled {} sequences from the test distribution.\".format(n_test))\n",
    "\n",
    "        # fetch and normalize weights of calibration data\n",
    "        p_sxm1 = np.hstack([np.tile(w_n[cal_idx], [n_test, 1]), w_n[test_idx[testsamp_idx]][:, None]])\n",
    "        p_sxm1 /= np.sum(p_sxm1, axis=1, keepdims=True)\n",
    "        \n",
    "        # compute quantile of weighted calibration scores\n",
    "        augscore_sxm1 = np.tile(np.hstack([score_n[cal_idx], [np.infty]]), (n_test, 1))\n",
    "        q_sx1 = cal.get_weighted_quantile(1 - alpha, p_sxm1.T, augscore_sxm1.T)[:, None]\n",
    "        \n",
    "        # construct confidence intervals\n",
    "        testpred_sx1 = pred_n[test_idx[testsamp_idx]][:, None]\n",
    "        lu_sx2 =  np.hstack([testpred_sx1 - q_sx1, testpred_sx1 + q_sx1])\n",
    "         \n",
    "        # record confidence interval lengths, true fitnesses, and empirical coverage\n",
    "        noninf_idx = np.where(np.logical_and(~np.isinf(lu_sx2[:, 0]), ~np.isinf(lu_sx2[:, 1])))[0]\n",
    "        avglen_lxt[l, t] = np.mean(2 * q_sx1[noninf_idx]) if noninf_idx.size else np.nan\n",
    "        fracinf_lxt[l, t] = (n_test - noninf_idx.size) / n_test\n",
    "        len_lxt[(l, t)] = 2 * q_sx1.flatten()\n",
    "        fit_lxt[(l, t)] = y_n[test_idx[testsamp_idx]]\n",
    "        cov_lxt[l, t] = cal.get_split_coverage(lu_sx2, fit_lxt[(l, t)])\n",
    "        \n",
    "    print(\"  Empirical coverage: {:.4f}\\n  Average non-inf length: {:.2f}\\n  Fraction inf: {:.2f}\\n  ({:.1f} s)\".format(\n",
    "        np.mean(cov_lxt[l]), np.nanmean(avglen_lxt[l]), np.mean(fracinf_lxt[l]), time.time() - t0))\n",
    "    \n",
    "    # save results after each lambda\n",
    "    if save_results:\n",
    "        np.savez(savefile, cov_lxt=cov_lxt, avglen_lxt=avglen_lxt,\n",
    "                 fracinf_lxt=fracinf_lxt, len_lxt=len_lxt, fit_lxt=fit_lxt)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ditto, but with randomized staircase confidence sets to achieve exact coverage (Fig. 5)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test distribution with lambda = 7.0\n",
      "  On trial 0, sampled 6 sequences from the test distribution.\n",
      "  Average non-inf length 5.80\n",
      "  Fraction inf 0.17\n",
      "109.6 s\n",
      "Test distribution with lambda = 6.0\n",
      "  On trial 0, sampled 15 sequences from the test distribution.\n",
      "  Average non-inf length 5.49\n",
      "  Fraction inf 0.06\n",
      "150.0 s\n",
      "Test distribution with lambda = 5.0\n",
      "  On trial 0, sampled 40 sequences from the test distribution.\n",
      "  Average non-inf length 5.06\n",
      "  Fraction inf 0.01\n",
      "228.0 s\n",
      "Test distribution with lambda = 4.0\n",
      "  On trial 0, sampled 128 sequences from the test distribution.\n",
      "  Average non-inf length 4.81\n",
      "  Fraction inf 0.00\n",
      "338.8 s\n",
      "Test distribution with lambda = 3.0\n",
      "  On trial 0, sampled 472 sequences from the test distribution.\n",
      "  Average non-inf length 4.75\n",
      "  Fraction inf 0.00\n",
      "771.5 s\n",
      "Test distribution with lambda = 2.0\n",
      "  On trial 0, sampled 3233 sequences from the test distribution.\n",
      "  Average non-inf length 4.69\n",
      "  Fraction inf 0.00\n",
      "3919.5 s\n",
      "Test distribution with lambda = 1.0\n",
      "  On trial 0, sampled 7130 sequences from the test distribution.\n",
      "  Average non-inf length 4.79\n",
      "  Fraction inf 0.00\n",
      "7445.0 s\n"
     ]
    }
   ],
   "source": [
    "reload(cal)\n",
    "n_trial = 500\n",
    "alpha = 0.1\n",
    "n_cal = 10000\n",
    "save_results = True\n",
    "savefile = '../aav/randomized-staircase-results.npz'\n",
    "\n",
    "# compute training likelihoods of all sequences\n",
    "logptrain_n = assay.get_loglikelihood(seq_n, assay.PNNKAA_LXK)\n",
    "\n",
    "n_lambda = phitestnuc_lxLxk.shape[0]\n",
    "avglen_lxt = np.zeros([n_lambda, n_trial])\n",
    "fracinf_lxt = np.zeros([n_lambda, n_trial])\n",
    "len_lxt = {(l, t): None for l, t in zip(range(n_lambda), range(n_trial))}\n",
    "fit_lxt = {(l, t): None for l, t in zip(range(n_lambda), range(n_trial))}\n",
    "cov_lxt = {(l, t): None for l, t in zip(range(n_lambda), range(n_trial))}\n",
    "\n",
    "for l in range(n_lambda - 1, -1, -1):\n",
    "    t0 = time.time()\n",
    "    print(\"Test distribution with lambda = {:.1f}\".format(lambda_l[l]))\n",
    "    \n",
    "    # compute acceptance probabilities for all sequences (for rejection sampling from test distribution)\n",
    "    paccept_n, logptest_n = assay.get_rejection_sampling_acceptance_probabilities(\n",
    "        seq_n, phitestnuc_lxLxk[l], logptrain_n)\n",
    "\n",
    "    # compute (unnormalized) weights for all data\n",
    "    w_n = np.exp(logptest_n - logptrain_n)\n",
    "\n",
    "    for t in range(n_trial):\n",
    "        \n",
    "        # partition held-out data into calibration data and test data\n",
    "        # (i.e., samples from proposal distribution for rejection sampling from test distribution)\n",
    "        shuffle_idx = np.random.permutation(n)\n",
    "        cal_idx, test_idx = shuffle_idx[: n_cal], shuffle_idx[n_cal :]\n",
    "        \n",
    "        # sample from test distribution using rejection sampling\n",
    "        testsamp_idx = assay.rejection_sample_from_test_distribution(paccept_n[test_idx])\n",
    "        n_test = testsamp_idx.size\n",
    "        if t == 0:  # example of how many sequences are sampled from test distribution on a trial\n",
    "            print(\"  On trial 0, sampled {} sequences from the test distribution.\".format(n_test))\n",
    "\n",
    "        # fetch and normalize weights of calibration data\n",
    "        p_sxm1 = np.hstack([np.tile(w_n[cal_idx], [n_test, 1]), w_n[test_idx[testsamp_idx]][:, None]])\n",
    "        p_sxm1 /= np.sum(p_sxm1, axis=1, keepdims=True)\n",
    "        \n",
    "        # construct randomized staircase confidence set\n",
    "        testpred_s = pred_n[test_idx[testsamp_idx]]\n",
    "        C_s = [cal.get_randomized_staircase_confidence_set(\n",
    "            score_n[cal_idx], weights_m1, pred, alpha) for weights_m1, pred in zip(p_sxm1, testpred_s)]\n",
    "         \n",
    "        # record true fitnesses, empirical coverage, confidence set sizes\n",
    "        fit_lxt[(l, t)] = y_n[test_idx[testsamp_idx]]\n",
    "        cov_s, len_s = cal.get_randomized_staircase_coverage(C_s, fit_lxt[(l, t)])\n",
    "        cov_lxt[(l, t)] = cov_s\n",
    "        noninf_idx = np.where(~np.isinf(len_s))[0]\n",
    "        avglen_lxt[l, t] = np.mean(len_s[noninf_idx]) if noninf_idx.size else np.nan\n",
    "        fracinf_lxt[l, t] = (n_test - noninf_idx.size) / n_test\n",
    "        len_lxt[(l, t)] = len_s\n",
    "    \n",
    "    cov = np.mean([np.mean(cov_lxt[(l, t)]) for t in range(n_trial)])\n",
    "    print(\"  Empirical coverage: {:.4f}\\n  Average non-inf length: {:.2f}\\n  Fraction inf: {:.2f}\\n  ({:.1f} s)\".format(\n",
    "        cov, np.nanmean(avglen_lxt[l]), np.mean(fracinf_lxt[l]), time.time() - t0))\n",
    "        \n",
    "    # save results after each lambda\n",
    "    if save_results:\n",
    "        np.savez(savefile, cov_lxt=cov_lxt, avglen_lxt=avglen_lxt,\n",
    "                 fracinf_lxt=fracinf_lxt, len_lxt=len_lxt, fit_lxt=fit_lxt)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "TensorFlow-GPU-2.1.0",
   "language": "python",
   "name": "tf-gpu"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
